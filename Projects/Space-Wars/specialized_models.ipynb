{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on Minimally Processed Text\n",
    "## Part 00: TF-IDF + NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# # Get a sorted list of the objects and their sizes\n",
    "# sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"Data/df.pkl\", 'rb') as picklefile:\n",
    "    df = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do our analysis on stemmed words so that same topic is not split (e.g. \"word\" and \"words\" should belong to the same topic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from textblob import TextBlob\n",
    "# stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "# def stem_getter(text):\n",
    "#     return \" \".join([stemmer.stem(word) for word in TextBlob(text).words])\n",
    "\n",
    "# df.raw_text = df.raw_text.map(stem_getter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around, let's remove words in all caps: they are used to indicate character lines. Using them will just create topics identifying major characters of a show/movie which is not helpful. Let's also remove non-letter characters along the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cap_remover(text):\n",
    "    text = re.sub(r'[A-Z]+(?![a-z])', '', text)\n",
    "    text = re.sub(r'[\\d]+', '', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    return re.sub(r\"[^\\w' ]\", '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.raw_text = df.raw_text.map(cap_remover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def stopword_remover(text):\n",
    "#     return \" \".join([word for word in text.lower().split() if word not in stoplist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df.raw_text = df.raw_text.map(stopword_remover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.raw_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "# Import all of the scikit learn stuff\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = df.raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have also used a large list of keywords from [here](http://www.ranks.nl/stopwords) and supplemented it with Star Trek specific terms discovered in the initial LDA model so that they are not used in topic analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Data/stopwords.txt') as f:\n",
    "     content = (f.read()).split()#f.readlines()\n",
    "# for i in range(len(content)):\n",
    "#     content[i] = content[i].replace(\"\\n\", \"\").lower()\n",
    "stoplist = sorted(list(set(content)))\n",
    "# texts = [\" \".join([word for word in cluster_dict[i].lower().split() if word not in stoplist])\n",
    "#          for i in cluster_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stoplist, ngram_range=(1, 2))\n",
    "dtm = vectorizer.fit_transform(docs) \n",
    "# pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's search for th optimal number of topics to explore (e.g. search until topics start make sense):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics for 30 topic NMF are:\n",
      "['viewscreen', 'warp', 'vulcan', 'time', 'sir', 'going', 'shuttlepod', 'bridge', 'ship', 'suliban']\n",
      "['door', 'console', 'cardassian', 'turns', 'time', 'moment', 'smiles', 'reacts', 'moves', 'beat']\n",
      "['drones', 'drone', 'assimilated', 'cube', 'will', 'descent', 'hugh', 'borg ship', 'collective', 'borg']\n",
      "['iklan', 'omet', 'omet iklan', 'shapeshifter', 'female shapeshifter', 'muniz', 'dominion', 'jem', 'hadar', 'jem hadar']\n",
      "['cards', 'kirby', 'giger', 'shattered mirror', 'lwaxana', 'beat', 'visitor', 'explorers', 'muse', 'jake']\n",
      "['bridge', 'ship', 'council', 'sphere', 'degra ship', 'reptilians', 'weapon', 'reptilian', 'xindi', 'degra']\n",
      "['well', 'ship', 'doctor', 'mister scott', 'planet', 'will', 'bridge', 'sir', 'scott', 'mister']\n",
      "['ishka', 'ferengi love', 'profit', 'du', 'brunt', 'maihar', 'maihar du', 'ferengi', 'zek', 'nagus']\n",
      "['ross', 'ezri', 'kasidy', 'solbor', 'prophets', 'dominion', 'breen', 'winn', 'weyoun', 'damar']\n",
      "['chain', 'madred', 'sarek', 'vin', 'chain pt', 'jellico', 'unification', 'tense pt', 'unification pt', 'pt']\n",
      "['empire', 'kahless', 'ch', 'grilka', 'apocalypse', 'klingons', 'warrior', 'gowron', 'klingon', 'martok']\n",
      "['irina', 'command module', 'flyer aft', 'flyer cockpit', 'ares', 'time', 'slipstream', 'delta', 'delta flyer', 'flyer']\n",
      "['ilario', 'norvo', 'janel', 'emperor', 'prodigal', 'joran', 'field fire', 'penumbra', 'afterimage', 'ezri']\n",
      "['astrometrics', 'max', 'lab', 'tallera', 'bridge', 'gambit', 'baran', 'ransom', 'equinox bridge', 'equinox']\n",
      "['kai', 'fascination', 'kubus', 'bajoran', 'prophets', 'collaborator', 'resurrection', 'vedek', 'winn', 'bareil']\n",
      "['cardassians', 'sakonna', 'xhosa', 'adversary', 'kasidy', 'blaze', 'cardassian', 'hudson', 'maquis', 'eddington']\n",
      "['macduff', 'power play', 'kalita', 'beat', 'preemptive', 'rascals', 'preemptive strike', 'conundrum', 'ensign ro', 'ro']\n",
      "['wormhole', 'reacts', 'hunt', 'alien', 'beat', 'hunter', 'pursuit', 'captive', 'captive pursuit', 'tosk']\n",
      "['favourite food', 'amplifier', 'natalie', 'jupiter', 'jupiter station', 'sir', 'cannons', 'bridge', 'monitor', 'armoury']\n",
      "['mister', 'bridge', 'suder', 'ship', 'nistrim', 'trabe', 'maje', 'kazon ship', 'culluh', 'kazon']\n",
      "['guy', 'love', 'jeanluc', 'qpid', 'vorgons', 'uthat', 'sovak', 'sir guy', 'holiday', 'vash']\n",
      "['bioweapon', 'chemist shop', 'rat', 'quarters', 'barge twelve', 'brig', 'chemist', 'trellium', 'xindi', 'rajiin']\n",
      "['time', 'well', 'going', 'ship', 'will', 'holodeck', 'bridge', 'sickbay', 'doctor', 'programme']\n",
      "['beat', 'mpec', 'mogh', 'sins father', 'klingon', 'ehleyr', 'gowron', 'duras', 'redemption', 'kurn']\n",
      "['survivors', 'cage', 'sir', 'hearing room', 'talosians', 'talosian', 'illusion', 'talos', 'vina', 'pike']\n",
      "['ezri', 'zeemo', 'leeta', 'cicci', 'cane', 'holosuite', 'shakaar', 'frankie', 'baddabing', 'vic']\n",
      "['toq', 'oath', 'darok', 'koloth', 'martok', 'blood oath', 'kang', 'birthright', 'sword', 'kor']\n",
      "['soval', 'syrrannites', 'surak', 'ambassador', 'high command', 'andorian', 'vulcans', 'andorians', 'vulcan', 'shran']\n",
      "['thirtyone', 'extreme', 'measures', 'koval', 'inter', 'arma', 'inter arma', 'inquisition', 'extreme measures', 'sloan']\n",
      "['commander', 'viewer', 'bridge', 'turns', 'beat', 'will', 'ship', 'pulaski', 'continuing', 'sir']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num_topics in range(2,11):\n",
    "    nmf_model = NMF(num_topics)\n",
    "\n",
    "    dtm_nmf = nmf_model.fit_transform(dtm)\n",
    "\n",
    "    # use NMF to attempt Topic Modeling\n",
    "    words = vectorizer.get_feature_names()\n",
    "\n",
    "    # get num_topic_words top topic words:\n",
    "    num_topic_words = 10\n",
    "\n",
    "    # iterate through our eigenvectors\n",
    "    print(\"The topics for {} topic NMF are:\".format(num_topics))\n",
    "    for r in nmf_model.components_:\n",
    "        # sort values associated with each dimension \n",
    "        a=sorted([(v,i) for i,v in enumerate(r)])[-num_topic_words:]\n",
    "        # map back to words\n",
    "        print([words[i[1]] for i in a])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like having # topics in NMF model makes the most sense to me. The as far as I can tell are topics:\n",
    "1. \n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf_model = NMF(5)\n",
    "\n",
    "dtm_nmf = nmf_model.fit_transform(dtm)\n",
    "# dtm_nmf = Normalizer(copy=False).fit_transform(dtm_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(678, 49838)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 678 documents and the matrix has thousands and thousands of words associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.13,  0.01,  0.  ,  0.  ],\n",
       "       [ 0.03,  0.18,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.02,  0.13,  0.  ,  0.  ,  0.  ],\n",
       "       ..., \n",
       "       [ 0.01,  0.12,  0.07,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.15,  0.03,  0.  ,  0.  ],\n",
       "       [ 0.01,  0.12,  0.04,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_nmf[:,:5].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.01,  0.  ,  0.  ,  0.  ,  0.07],\n",
       "       ..., \n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.01]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_nmf[:,5:10].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the NMF-transformed TF-IDF matrices above, there are no strong topics emerging out of the 10 columns. Therefore, I shall attempt to perform K-means clustering to classify the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(678, 10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_nmf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have two topics now (two columns in the NMF transformed matrix). And the components (word distribution among the two topics) of those two topics are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.33850614e-03,   0.00000000e+00,   4.53588539e-05, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.94387040e-05],\n",
       "       [  0.00000000e+00,   3.36421378e-04,   3.55596430e-04, ...,\n",
       "          0.00000000e+00,   1.68352417e-05,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   2.67690335e-05,   2.80993368e-05, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.42787006e-04],\n",
       "       ..., \n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model.components_ #[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 49838)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just assign each document to its highest feature, a clustering, in a sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nmf_data_clust=[list(in_list).index(max(in_list)) for in_list in dtm_nmf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "678"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmf_data_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well', 'will', 'doctor', 'mister', 'don', 'll', 've']\n",
      "['time', 'going', 'weyoun', 've', 'll', 'don', 'beat']\n",
      "['time', 'going', 'degra', 'trip', 'don', 'll', 've']\n",
      "['reacts', 'turns', 'console', 'will', 'moves', 'continuing', 'beat']\n",
      "['will', 'cube', 'collective', 'll', 've', 'delta', 'flyer']\n"
     ]
    }
   ],
   "source": [
    "# 4) use NMF to attempt Topic Modeling\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "## get 7 top topic words:\n",
    "\n",
    "# iterate through our eigenvectors\n",
    "for r in nmf_model.components_:\n",
    "    # sort values associated with each dimension \n",
    "    a=sorted([(v,i) for i,v in enumerate(r)])[-7:]\n",
    "    # map back to words\n",
    "    print([words[i[1]] for i in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 1 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-753bd5089bbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtm_nmf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtm_nmf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for axis 1 with size 10"
     ]
    }
   ],
   "source": [
    "x = dtm_nmf[:,0]\n",
    "y = dtm_nmf[:,10]\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 01: Word Count + LDA  \n",
    "I am going to follow the guide [here](http://www.shichaoji.com/category/data-cleasing/) for (hopefully) a better topic visuzlization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helper import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation) \n",
    "lemmatize = WordNetLemmatizer()\n",
    "\n",
    "def cleaning(article):\n",
    "    one = \" \".join([i for i in article.lower().split() if i not in stopwords])\n",
    "    two = \"\".join(i for i in one if i not in punctuation)\n",
    "    three = \" \".join(lemmatize.lemmatize(i) for i in two.split())\n",
    "    return three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df.drop([\"_id\",\"end\",\"series\",\"start\",\"url\",\"airdate\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = df2.applymap(cleaning)['raw_text']\n",
    "text_list = [i.split() for i in text]\n",
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO,\n",
    "                   filename='running.log',filemode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = None\n",
    "df2 = None\n",
    "docs = None\n",
    "del df\n",
    "del df2\n",
    "del docs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gc.get_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "dictionary = corpora.Dictionary(text_list)\n",
    "dictionary.save('dictionary.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_list]\n",
    "corpora.MmCorpus.serialize('corpus.mm', doc_term_matrix)\n",
    "\n",
    "# print(len(doc_term_matrix))\n",
    "# print(doc_term_matrix[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word = dictionary, passes=50)\n",
    "print('used: {:.2f}s'.format(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"Data/ldamodel1.pkl\", 'wb') as picklefile:\n",
    "    pickle.dump(ldamodel, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in ldamodel.print_topics(): \n",
    "    for j in i: print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brilliant! My topics are based on characters that are in the respective series! Not a very useful information, but it LDA works pretty well. Let's see if we can visuzlize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel.save('topic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "loading = LdaModel.load('topic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(loading.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_new(doc):\n",
    "    one = cleaning(doc).split()\n",
    "    two = dictionary.doc2bow(one)\n",
    "    return two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import gensim\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = gensim.corpora.Dictionary.load('dictionary.dict')\n",
    "c = gensim.corpora.MmCorpus('corpus.mm')\n",
    "lda = gensim.models.LdaModel.load('topic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pyLDAvis.gensim.prepare(lda, c, d)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(data,'vis1.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty good separation. Unfortunately, it is based mostly on very specific TV show / movies terms. Therefore, I will need to remove those if I want better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
