{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 00: TF-IDF + NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# # Get a sorted list of the objects and their sizes\n",
    "# sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"Data/df.pkl\", 'rb') as picklefile:\n",
    "    df = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do our analysis on stemmed words so that same topic is not split (e.g. \"word\" and \"words\" should belong to the same topic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from textblob import TextBlob\n",
    "# stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "# def stem_getter(text):\n",
    "#     return \" \".join([stemmer.stem(word) for word in TextBlob(text).words])\n",
    "\n",
    "# df.raw_text = df.raw_text.map(stem_getter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around, let's remove words in all caps: they are used to indicate character lines. Using them will just create topics identifying major characters of a show/movie which is not helpful. Let's also remove non-letter characters along the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cap_remover(text):\n",
    "    text = re.sub(r'[A-Z]+(?![a-z])', '', text)\n",
    "    text = re.sub(r'[\\d]+', '', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    return re.sub(r\"[^\\w' ]\", '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.raw_text = df.raw_text.map(cap_remover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def stopword_remover(text):\n",
    "#     return \" \".join([word for word in text.lower().split() if word not in stoplist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df.raw_text = df.raw_text.map(stopword_remover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.raw_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "# Import all of the scikit learn stuff\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = df.raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have also used a large list of keywords from [here](http://www.ranks.nl/stopwords) and supplemented it with Star Trek specific terms discovered in the initial LDA model so that they are not used in topic analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Data/stopwords.txt') as f:\n",
    "     content = (f.read()).split()#f.readlines()\n",
    "# for i in range(len(content)):\n",
    "#     content[i] = content[i].replace(\"\\n\", \"\").lower()\n",
    "stoplist = sorted(list(set(content)))\n",
    "# texts = [\" \".join([word for word in cluster_dict[i].lower().split() if word not in stoplist])\n",
    "#          for i in cluster_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stoplist, ngram_range=(1, 3))\n",
    "dtm = vectorizer.fit_transform(docs) \n",
    "# pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's search for th optimal number of topics to explore (e.g. search until topics start make sense):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num_topics in range(40,60,10):\n",
    "    nmf_model = NMF(num_topics)\n",
    "\n",
    "    dtm_nmf = nmf_model.fit_transform(dtm)\n",
    "\n",
    "    # use NMF to attempt Topic Modeling\n",
    "    words = vectorizer.get_feature_names()\n",
    "\n",
    "    # get num_topic_words top topic words:\n",
    "    num_topic_words = 30\n",
    "\n",
    "    # iterate through our eigenvectors\n",
    "    print(\"The topics for {} topic NMF are:\".format(num_topics))\n",
    "    for r in nmf_model.components_:\n",
    "        # sort values associated with each dimension \n",
    "        a=sorted([(v,i) for i,v in enumerate(r)])[-num_topic_words:]\n",
    "        # map back to words\n",
    "        print([words[i[1]] for i in a])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like having # topics in NMF model makes the most sense to me. The as far as I can tell are topics:\n",
    "1. \n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf_model = NMF(5)\n",
    "\n",
    "dtm_nmf = nmf_model.fit_transform(dtm)\n",
    "# dtm_nmf = Normalizer(copy=False).fit_transform(dtm_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(678, 49838)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 678 documents and the matrix has thousands and thousands of words associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.13,  0.01,  0.  ,  0.  ],\n",
       "       [ 0.03,  0.18,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.02,  0.13,  0.  ,  0.  ,  0.  ],\n",
       "       ..., \n",
       "       [ 0.01,  0.12,  0.07,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.15,  0.03,  0.  ,  0.  ],\n",
       "       [ 0.01,  0.12,  0.04,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_nmf[:,:5].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.01,  0.  ,  0.  ,  0.  ,  0.07],\n",
       "       ..., \n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.01]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_nmf[:,5:10].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the NMF-transformed TF-IDF matrices above, there are no strong topics emerging out of the 10 columns. Therefore, I shall attempt to perform K-means clustering to classify the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(678, 10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_nmf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have two topics now (two columns in the NMF transformed matrix). And the components (word distribution among the two topics) of those two topics are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.33850614e-03,   0.00000000e+00,   4.53588539e-05, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.94387040e-05],\n",
       "       [  0.00000000e+00,   3.36421378e-04,   3.55596430e-04, ...,\n",
       "          0.00000000e+00,   1.68352417e-05,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   2.67690335e-05,   2.80993368e-05, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.42787006e-04],\n",
       "       ..., \n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model.components_ #[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 49838)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just assign each document to its highest feature, a clustering, in a sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nmf_data_clust=[list(in_list).index(max(in_list)) for in_list in dtm_nmf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "678"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmf_data_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well', 'will', 'doctor', 'mister', 'don', 'll', 've']\n",
      "['time', 'going', 'weyoun', 've', 'll', 'don', 'beat']\n",
      "['time', 'going', 'degra', 'trip', 'don', 'll', 've']\n",
      "['reacts', 'turns', 'console', 'will', 'moves', 'continuing', 'beat']\n",
      "['will', 'cube', 'collective', 'll', 've', 'delta', 'flyer']\n"
     ]
    }
   ],
   "source": [
    "# 4) use NMF to attempt Topic Modeling\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "## get 7 top topic words:\n",
    "\n",
    "# iterate through our eigenvectors\n",
    "for r in nmf_model.components_:\n",
    "    # sort values associated with each dimension \n",
    "    a=sorted([(v,i) for i,v in enumerate(r)])[-7:]\n",
    "    # map back to words\n",
    "    print([words[i[1]] for i in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
